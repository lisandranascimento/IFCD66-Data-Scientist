{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Web-Scraping-Lab\" data-toc-modified-id=\"Web-Scraping-Lab-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Web Scraping Lab</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Useful-Resources\" data-toc-modified-id=\"Useful-Resources-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Useful Resources</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-of-all,-gathering-our-tools.\" data-toc-modified-id=\"First-of-all,-gathering-our-tools.-1.0.1.1\"><span class=\"toc-item-num\">1.0.1.1&nbsp;&nbsp;</span>First of all, gathering our tools.</a></span></li><li><span><a href=\"#Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:\" data-toc-modified-id=\"Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:-1.0.1.2\"><span class=\"toc-item-num\">1.0.1.2&nbsp;&nbsp;</span>Challenge 1 - Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:</a></span></li><li><span><a href=\"#Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.\" data-toc-modified-id=\"Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.-1.0.1.3\"><span class=\"toc-item-num\">1.0.1.3&nbsp;&nbsp;</span>Display the names of the trending developers retrieved in the previous step.</a></span></li><li><span><a href=\"#Challenge-2---Display-the-trending-Python-repositories-in-GitHub\" data-toc-modified-id=\"Challenge-2---Display-the-trending-Python-repositories-in-GitHub-1.0.1.4\"><span class=\"toc-item-num\">1.0.1.4&nbsp;&nbsp;</span>Challenge 2 - Display the trending Python repositories in GitHub</a></span></li><li><span><a href=\"#Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page\" data-toc-modified-id=\"Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page-1.0.1.5\"><span class=\"toc-item-num\">1.0.1.5&nbsp;&nbsp;</span>Challenge 3 - Display all the image links from Walt Disney wikipedia page</a></span></li><li><span><a href=\"#Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.\" data-toc-modified-id=\"Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.-1.0.1.6\"><span class=\"toc-item-num\">1.0.1.6&nbsp;&nbsp;</span>Challenge 4 - Retrieve all links to pages on Wikipedia that refer to some kind of Python.</a></span></li><li><span><a href=\"#Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point\" data-toc-modified-id=\"Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point-1.0.1.7\"><span class=\"toc-item-num\">1.0.1.7&nbsp;&nbsp;</span>Challenge 5 - Number of Titles that have changed in the United States Code since its last release point</a></span></li><li><span><a href=\"#Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names\" data-toc-modified-id=\"Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names-1.0.1.8\"><span class=\"toc-item-num\">1.0.1.8&nbsp;&nbsp;</span>Challenge 6 - A Python list with the top ten FBI's Most Wanted names</a></span></li><li><span><a href=\"#Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org\" data-toc-modified-id=\"Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org-1.0.1.9\"><span class=\"toc-item-num\">1.0.1.9&nbsp;&nbsp;</span>Challenge 7 - List all language names and number of related articles in the order they appear in wikipedia.org</a></span></li><li><span><a href=\"#Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk\" data-toc-modified-id=\"Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk-1.0.1.10\"><span class=\"toc-item-num\">1.0.1.10&nbsp;&nbsp;</span>Challenge 8 - A list with the different kind of datasets available in data.gov.uk</a></span></li><li><span><a href=\"#Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe\" data-toc-modified-id=\"Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe-1.0.1.11\"><span class=\"toc-item-num\">1.0.1.11&nbsp;&nbsp;</span>Challenge 9 - Top 10 languages by number of native speakers stored in a Pandas Dataframe</a></span></li></ul></li><li><span><a href=\"#Stepping-up-the-game\" data-toc-modified-id=\"Stepping-up-the-game-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Stepping up the game</a></span><ul class=\"toc-item\"><li><span><a href=\"#Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe-1.0.2.1\"><span class=\"toc-item-num\">1.0.2.1&nbsp;&nbsp;</span>Challenge 10 - The 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe-1.0.2.2\"><span class=\"toc-item-num\">1.0.2.2&nbsp;&nbsp;</span>Challenge 11 - IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.-1.0.2.3\"><span class=\"toc-item-num\">1.0.2.3&nbsp;&nbsp;</span>Challenge 12 - Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe.</a></span></li><li><span><a href=\"#Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.\" data-toc-modified-id=\"Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.-1.0.2.4\"><span class=\"toc-item-num\">1.0.2.4&nbsp;&nbsp;</span>Challenge 13 - Find the live weather report (temperature, wind speed, description and weather) of a given city.</a></span></li><li><span><a href=\"#Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.-1.0.2.5\"><span class=\"toc-item-num\">1.0.2.5&nbsp;&nbsp;</span>Challenge 14 - Book name,price and stock availability as a pandas dataframe.</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio de Web Scraping\n",
    "\n",
    "Encontrarás en este cuaderno algunos ejercicios de web scraping para practicar tus habilidades de scraping usando `requests` y `Beautiful Soup`.\n",
    "\n",
    "**Consejos:**\n",
    "\n",
    "- Verifica el [código de estado de la respuesta](https://http.cat/) para cada solicitud para asegurarte de haber obtenido el contenido previsto.\n",
    "- Observa el código HTML en cada solicitud para entender el tipo de información que estás obteniendo y su formato.\n",
    "- Busca patrones en el texto de respuesta para extraer los datos/información solicitados en cada pregunta.\n",
    "- Visita cada URL y echa un vistazo a su fuente a través de Chrome DevTools. Necesitarás identificar las etiquetas HTML, nombres de clases especiales, etc., utilizados para el contenido HTML que se espera extraer.\n",
    "- Revisa los selectores CSS.\n",
    "\n",
    "### Recursos Útiles\n",
    "- Documentación de la [biblioteca Requests](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Doc de Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Lista de códigos de estado HTTP](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [Conceptos básicos de HTML](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [Conceptos básicos de CSS](https://www.cssbasics.com/#page_start)\n",
    "\n",
    "#### Primero que todo, reuniendo nuestras herramientas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Nuevamente, recuerda limitar tu salida antes de la entrega para que tu código no se pierda en la salida.**\n",
    "\n",
    "#### Desafío 1 - Descargar, analizar (usando BeautifulSoup) e imprimir el contenido de la página de Desarrolladores en Tendencia de GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_html=requests.get(url).text\n",
    "soup = BeautifulSoup(github_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muestra los nombres de los desarrolladores en tendencia recuperados en el paso anterior.\n",
    "\n",
    "Tu salida debe ser una lista de Python con los nombres de los desarrolladores. Cada nombre no debe contener ninguna etiqueta HTML.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. Descubre la etiqueta HTML y los nombres de clase usados para los nombres de los desarrolladores. Puedes lograr esto usando Chrome DevTools.\n",
    "\n",
    "1. Usa BeautifulSoup para extraer todos los elementos HTML que contienen los nombres de los desarrolladores.\n",
    "\n",
    "1. Utiliza técnicas de manipulación de cadenas para reemplazar espacios en blanco y saltos de línea (es decir, `\\n`) en el *texto* de cada elemento HTML. Usa una lista para almacenar los nombres limpios.\n",
    "\n",
    "1. Imprime la lista de nombres.\n",
    "\n",
    "Tu salida debería lucir como abajo (con nombres diferentes):\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "# Encontrar los elementos HTML que contienen los nombres de los desarrolladores\n",
    "developer_elements = soup.find_all('h2', class_='h3 lh-condensed')\n",
    "\n",
    "# Extraer y limpiar los nombres de los desarrolladores\n",
    "developers = []\n",
    "for element in developer_elements:\n",
    "    name = element.get_text(strip=True)\n",
    "    developers.append(name)\n",
    "\n",
    "# Imprimir la lista de nombres de desarrolladores\n",
    "print(developers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Casper (casper-hansen)', 'Marc Seitz (mfts)', 'George Hotz (geohot)', 'Estelle Weyl (estelle)', 'Ben V. Brown (Ralim)', 'Stephen Celis (stephencelis)', 'Brace Sproul (bracesproul)', 'Luigi Ballabio (lballabio)', 'Shane Neuville (PureWeen)', 'doronz88 (mikavilpas)', 'Mika Vilpas (takatost)', 'takatost (sue445)', 'Go Sueyoshi (CatsJuice)', 'Cats Juice (youknowriad)', 'Riad Benguella (logan-markewich)', 'Logan (alexey-milovidov)', 'Alexey Milovidov (lucidrains)', 'Phil Wang (MaskRay)', 'Fangrui Song (awni)', 'Awni Hannun (abhinav)', 'Abhinav Gupta (avik-pal)', 'Avik Pal (kylebarron)', 'Kyle Barron (devongovett)', 'Devon Govett (arvinxx)']\n"
     ]
    }
   ],
   "source": [
    "# Encontrar los elementos HTML que contienen los nombres de los desarrolladores y los nombres de usuario\n",
    "developer_names = soup.find_all('h1', class_='h3 lh-condensed')\n",
    "developer_usernames = soup.find_all('p', class_='f4 text-normal mb-1')\n",
    "\n",
    "# Extraer y limpiar los nombres de los desarrolladores\n",
    "developers = []\n",
    "for name, username in zip(developer_names, developer_usernames):\n",
    "    clean_name = name.get_text(strip=True)\n",
    "    clean_username = username.get_text(strip=True)\n",
    "    developers.append(f'{clean_name} ({clean_username})')\n",
    "\n",
    "# Imprimir la lista de nombres de desarrolladores\n",
    "print(developers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 2 - Mostrar los repositorios de Python en tendencia en GitHub\n",
    "\n",
    "Los pasos para resolver este problema son similares al anterior, excepto que necesitas encontrar los nombres de los repositorios en lugar de los nombres de los desarrolladores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la página de repositorios de Python en tendencia en GitHub\n",
    "url2 = 'https://github.com/trending/python?since=daily'\n",
    "\n",
    "# Obtener el contenido HTML de la página\n",
    "response = requests.get(url2)\n",
    "html_content = response.text\n",
    "\n",
    "# Parsear el contenido HTML usando BeautifulSoup\n",
    "soup2 = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['public-apis / public-apis', 'meta-llama / codellama', 'mem0ai / mem0', 'meta-llama / llama3', 'spipm / Depix', 'modelscope / agentscope', 'sinaatalay / rendercv', 'BerriAI / litellm', 'QuivrHQ / quivr', 'ComposioHQ / composio', 'pytest-dev / pytest', 'danielmiessler / fabric', 'opendatalab / MinerU', 'pydantic / pydantic']\n"
     ]
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "# Encontrar los elementos HTML que contienen los nombres de los repositorios\n",
    "repo_elements = soup2.find_all('h2', class_='h3 lh-condensed')\n",
    "\n",
    "# Extraer y limpiar los nombres de los repositorios\n",
    "repos = []\n",
    "for element in repo_elements:\n",
    "    repo_name = element.get_text(strip=True).replace(' ', '').replace('\\n', '').replace('/', ' / ')\n",
    "    repos.append(repo_name)\n",
    "\n",
    "# Imprimir la lista de nombres de repositorios\n",
    "print(repos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 3 - Mostrar todos los enlaces de imágenes de la página de Wikipedia de Walt Disney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "disney = requests.get(f\"{url3}\").text\n",
    "soup3 = BeautifulSoup(disney, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/static/images/icons/wikipedia.png', 'https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-wordmark-en.svg', 'https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-tagline-en.svg', 'https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG', 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg/220px-Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Nuvola_apps_kaboodle.svg/16px-Nuvola_apps_kaboodle.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Disney_Oscar_1953_%28cropped%29.jpg/170px-Disney_Oscar_1953_%28cropped%29.jpg', 'https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/20px-Commons-logo.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/23px-Wikiquote-logo.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/26px-Wikisource-logo.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/20px-Animation_disc.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/19px-P_vip.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Mickey_Mouse_colored_%28head%29.svg/20px-Mickey_Mouse_colored_%28head%29.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/19px-Video-x-generic.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/21px-Flag_of_Los_Angeles_County%2C_California.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/21px-Blank_television_set.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/21px-Flag_of_the_United_States.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png', 'https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1', 'https://en.wikipedia.org/static/images/footer/wikimedia-button.svg', 'https://en.wikipedia.org/static/images/footer/poweredby_mediawiki.svg']\n"
     ]
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "# Encontrar todos los elementos <img>\n",
    "img_elements = soup3.find_all('img')\n",
    "\n",
    "# Extraer y limpiar los enlaces de las imágenes\n",
    "img_urls = []\n",
    "for img in img_elements:\n",
    "    img_url = img.get('src')\n",
    "    if img_url:\n",
    "        # Completar el URL si es relativo\n",
    "        if img_url.startswith('//'):\n",
    "            img_url = 'https:' + img_url\n",
    "        elif img_url.startswith('/'):\n",
    "            img_url = 'https://en.wikipedia.org' + img_url\n",
    "        img_urls.append(img_url)\n",
    "\n",
    "# Imprimir la lista de URLs de imágenes\n",
    "print(img_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 4 - Recuperar todos los enlaces a páginas en Wikipedia que se refieren a algún tipo de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url4 ='https://en.wikipedia.org/wiki/Python' \n",
    "python = requests.get(f\"{url4}\").text\n",
    "soup4 = BeautifulSoup(python, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Pythonidae', 'https://en.wikipedia.org/wiki/Python_(genus)', 'https://en.wikipedia.org/wiki/Python_(mythology)', 'https://en.wikipedia.org/wiki/Python_(programming_language)', 'https://en.wikipedia.org/wiki/Python_of_Aenus', 'https://en.wikipedia.org/wiki/Python_(painter)', 'https://en.wikipedia.org/wiki/Python_of_Byzantium', 'https://en.wikipedia.org/wiki/Python_of_Catana', 'https://en.wikipedia.org/wiki/Python_Anghelo', 'https://en.wikipedia.org/wiki/Python_(Efteling)', 'https://en.wikipedia.org/wiki/Python_(Busch_Gardens_Tampa_Bay)', 'https://en.wikipedia.org/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)', 'https://en.wikipedia.org/wiki/Python_(automobile_maker)', 'https://en.wikipedia.org/wiki/Python_(Ford_prototype)', 'https://en.wikipedia.org/wiki/Python_(missile)', 'https://en.wikipedia.org/wiki/Python_(nuclear_primary)', 'https://en.wikipedia.org/wiki/Colt_Python', 'https://en.wikipedia.org/wiki/Python_(codename)', 'https://en.wikipedia.org/wiki/Python_(film)', 'https://en.wikipedia.org/wiki/Monty_Python', 'https://en.wikipedia.org/wiki/Python_(Monty)_Pictures']\n"
     ]
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "# Encontrar todos los elementos <a> que contienen enlaces\n",
    "links = soup4.find_all('a')\n",
    "\n",
    "# Extraer y limpiar los enlaces\n",
    "python_links = []\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    if href and href.startswith('/wiki/') and 'Python' in link.get_text():\n",
    "        full_url = 'https://en.wikipedia.org' + href\n",
    "        python_links.append(full_url)\n",
    "\n",
    "# Imprimir la lista de URLs\n",
    "print(python_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 5 - Número de Títulos que han cambiado en el Código de los Estados Unidos desde su último punto de lanzamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de títulos que han cambiado: 1\n"
     ]
    }
   ],
   "source": [
    "# Obtener el contenido HTML de la página\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parsear el contenido HTML usando BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Encontrar todos los enlaces en la página\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# Filtrar y contar los títulos que han cambiado\n",
    "changed_titles = 0\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    if href and href.endswith('.pdf'):\n",
    "        changed_titles += 1\n",
    "\n",
    "# Imprimir el número de títulos que han cambiado\n",
    "print(f'Número de títulos que han cambiado: {changed_titles}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 6 - Una lista de Python con los diez nombres más buscados por el FBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url7 = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "# Obtener el contenido HTML de la página\n",
    "response = requests.get(url7)\n",
    "html_content = response.text\n",
    "\n",
    "# Parsear el contenido HTML usando BeautifulSoup\n",
    "soup7 = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Encontrar todos los elementos que contienen los nombres de las personas más buscadas\n",
    "wanted_tags = soup7.find_all('h3', class_='title')\n",
    "\n",
    "# Extraer y limpiar los nombres\n",
    "wanted_names = [tag.get_text(strip=True) for tag in wanted_tags]\n",
    "\n",
    "# Imprimir la lista de nombres\n",
    "print(wanted_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 7 - Listar todos los nombres de idiomas y el número de artículos relacionados en el orden en que aparecen en wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# This is the url you will scrape in this exercise\n",
    "url8 = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = requests.get(f\"{url8}\").text\n",
    "soup8 = BeautifulSoup(languages, 'html.parser')\n",
    "langlist = soup8.find_all(\"div\", {\"class\": f\"central-featured-lang\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting articles for ÙØ§Ø±Ø³Û: Û±Ù¬Û°Û°Û¶Ù¬Û°Û°Û°+ Ù",
      "ÙØ§ÙÙ\n",
      "English: 6847000 artículos\n",
      "æ¥æ¬èª: 1421000 artículos\n",
      "Deutsch: 2924000 artículos\n",
      "Ð ÑÑÑÐºÐ¸Ð¹: 1987000 artículos\n",
      "EspaÃ±ol: 1965000 artículos\n",
      "FranÃ§ais: 2621000 artículos\n",
      "ä¸­æ: 1429000 artículos\n",
      "Italiano: 1871000 artículos\n",
      "PortuguÃªs: 1128000 artículos\n",
      "[('English', 6847000), ('æ\\x97¥æ\\x9c¬èª\\x9e', 1421000), ('Deutsch', 2924000), ('Ð\\xa0Ñ\\x83Ñ\\x81Ñ\\x81ÐºÐ¸Ð¹', 1987000), ('EspaÃ±ol', 1965000), ('FranÃ§ais', 2621000), ('ä¸\\xadæ\\x96\\x87', 1429000), ('Italiano', 1871000), ('PortuguÃªs', 1128000)]\n"
     ]
    }
   ],
   "source": [
    "# Extraer y limpiar los datos\n",
    "languages_articles = []\n",
    "for lang in langlist:\n",
    "    language = lang.find(\"strong\").text\n",
    "    articles_text = lang.find(\"small\").text\n",
    "    \n",
    "    # Eliminar todos los caracteres no numéricos\n",
    "    articles = re.sub(r'[^\\d]', '', articles_text)\n",
    "    \n",
    "    try:\n",
    "        articles_int = int(articles)\n",
    "        languages_articles.append((language, articles_int))\n",
    "    except ValueError:\n",
    "        print(f\"Error converting articles for {language}: {articles_text}\")\n",
    "\n",
    "# Imprimir la lista de idiomas y el número de artículos\n",
    "for language, articles in languages_articles:\n",
    "    print(f\"{language}: {articles} artículos\")\n",
    "\n",
    "# Alternativamente, puedes almacenar la lista en una variable para su uso posterior\n",
    "print(languages_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 8 - Una lista con los diferentes tipos de conjuntos de datos disponibles en data.gov.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url82 = 'https://data.gov.uk/'\n",
    "dats = requests.get(f\"{url82}\")\n",
    "soup8 = BeautifulSoup(dats.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cookies to collect information\n",
      "View cookies\n",
      "change your cookie settings\n",
      "Business and economy\n",
      "Crime and justice\n",
      "Defence\n",
      "Education\n",
      "Environment\n",
      "Government\n",
      "Government spending\n",
      "Health\n",
      "Mapping\n",
      "Society\n",
      "Towns and cities\n",
      "Transport\n",
      "Digital service performance\n",
      "Government reference data\n",
      "['cookies to collect information', 'View cookies', 'change your cookie settings', 'Business and economy', 'Crime and justice', 'Defence', 'Education', 'Environment', 'Government', 'Government spending', 'Health', 'Mapping', 'Society', 'Towns and cities', 'Transport', 'Digital service performance', 'Government reference data']\n"
     ]
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "# Encontrar los elementos que contienen los tipos de conjuntos de datos\n",
    "# Asumiendo que los tipos de conjuntos de datos están en una lista o enlaces específicos\n",
    "# Se necesita inspeccionar la página para encontrar el patrón correcto\n",
    "data_types_elements = soup8.find_all(\"a\", {\"class\": \"govuk-link\"})\n",
    "\n",
    "# Extraer y limpiar los nombres de los tipos de conjuntos de datos\n",
    "data_types = [element.text.strip() for element in data_types_elements if element.text.strip()]\n",
    "\n",
    "# Imprimir la lista de tipos de conjuntos de datos\n",
    "for data_type in data_types:\n",
    "    print(data_type)\n",
    "\n",
    "# Alternativamente, puedes almacenar la lista en una variable para su uso posterior\n",
    "print(data_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 9 - Los 10 idiomas con más hablantes nativos almacenados en un DataFrame de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url9 = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "tenlang = requests.get(url9)\n",
    "soup9 = BeautifulSoup(tenlang.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Language  Number of Native Speakers (millions)\n",
      "0  Mandarin Chinese                                   947\n",
      "1           Spanish                                   462\n",
      "2           English                                   392\n",
      "3            Arabic                                   392\n",
      "4             Hindi                                   269\n",
      "5           Bengali                                   254\n",
      "6        Portuguese                                   231\n",
      "7           Russian                                   161\n",
      "8          Japanese                                   130\n",
      "9   Western Punjabi                                   100\n"
     ]
    }
   ],
   "source": [
    "# Encontrar la tabla que contiene la información sobre los idiomas\n",
    "table = soup9.find('table', {'class': 'wikitable sortable'})\n",
    "\n",
    "# Inicializar listas para almacenar los datos\n",
    "languages = []\n",
    "speakers = []\n",
    "\n",
    "# Estimación de la población mundial en millones\n",
    "population_millions = 7700  # 7700 millones\n",
    "\n",
    "# Extraer los datos de la tabla\n",
    "for row in table.find_all('tr')[1:11]:  # Saltar el encabezado de la tabla y tomar solo las primeras 10 filas\n",
    "    cols = row.find_all('td')\n",
    "    language = cols[1].text.strip()  # La columna de los nombres de idiomas\n",
    "    speaker_number = cols[2].text.strip()  # La columna de los números de hablantes nativos\n",
    "    speaker_number = speaker_number.split('[')[0]  # Eliminar las referencias en el número de hablantes\n",
    "    speaker_number = speaker_number.replace(',', '').replace('%', '').strip()  # Eliminar comas y porcentajes\n",
    "\n",
    "    try:\n",
    "        percentage = float(speaker_number)\n",
    "        estimated_speakers = int((percentage / 100) * population_millions)\n",
    "        speakers.append(estimated_speakers)\n",
    "        languages.append(language)\n",
    "    except ValueError:\n",
    "        print(f\"Error converting speaker number for {language}: {speaker_number}\")\n",
    "\n",
    "# Crear un DataFrame de Pandas con los datos extraídos\n",
    "data = {\n",
    "    'Language': languages,\n",
    "    'Number of Native Speakers (millions)': speakers\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subiendo el nivel\n",
    "#### Desafío 10 - La información de los 20 últimos terremotos (fecha, hora, latitud, longitud y nombre de la región) por el EMSC como un dataframe de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "#url7 = 'https://www.emsc-csem.org/Earthquake/'\n",
    "url7 = \"https://www.emsc-csem.org/#2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\lisan\\anaconda3\\lib\\site-packages (4.23.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from selenium) (0.26.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: outcome in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: idna in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\lisan\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configuración del navegador\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Ejecutar en modo headless\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Ruta al ejecutable del controlador de Chrome (asegúrate de ajustar esta ruta)\n",
    "# Reemplaza con la ruta de tu chromedriver\n",
    "service = Service('C:/path/to/chromedriver.exe')  # Para Windows\n",
    "# service = Service('/usr/local/bin/chromedriver')  # Para MacOS/Linux\n",
    "\n",
    "# Iniciar el navegador\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# URL de la página de EMSC\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "driver.get(url)\n",
    "\n",
    "# Esperar a que se cargue la página\n",
    "time.sleep(10)  # Ajusta el tiempo según sea necesario para asegurar que la página se cargue\n",
    "\n",
    "# Buscar la tabla con los terremotos\n",
    "earthquake_data = []\n",
    "try:\n",
    "    rows = driver.find_elements(By.CSS_SELECTOR, \".table tbody tr\")[:20]\n",
    "    for row in rows:\n",
    "        cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "        if len(cols) >= 5:\n",
    "            date_time = cols[0].text.strip()\n",
    "            latitude = cols[1].text.strip()\n",
    "            longitude = cols[2].text.strip()\n",
    "            magnitude = cols[3].text.strip()\n",
    "            region = cols[4].text.strip()\n",
    "            earthquake_data.append({\n",
    "                'Date_Time': date_time,\n",
    "                'Latitude': latitude,\n",
    "                'Longitude': longitude,\n",
    "                'Magnitude': magnitude,\n",
    "                'Region': region\n",
    "            })\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# Crear un DataFrame de Pandas con los datos extraídos\n",
    "df = pd.DataFrame(earthquake_data)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 11 - Datos del Top 250 de IMDB (nombre de la película, lanzamiento inicial, nombre del director y estrellas) como un dataframe de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url11 = 'https://www.imdb.com/chart/top'\n",
    "pelis = requests.get(url11)\n",
    "soup = BeautifulSoup(pelis.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title, Release Year, Director, Stars]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "\n",
    "# Encontrar todos los elementos con las películas del top 250\n",
    "movie_elements = soup.select('td.titleColumn')\n",
    "\n",
    "# Inicializar listas para almacenar los datos\n",
    "titles = []\n",
    "release_years = []\n",
    "directors_and_stars = []\n",
    "\n",
    "# Extraer la información de cada película\n",
    "for movie_element in movie_elements:\n",
    "    title = movie_element.a.text\n",
    "    release_year = movie_element.span.text.strip('()')\n",
    "    \n",
    "    # Navegar a la página de la película para obtener director y estrellas\n",
    "    movie_url = 'https://www.imdb.com' + movie_element.a['href']\n",
    "    movie_page = requests.get(movie_url)\n",
    "    movie_soup = BeautifulSoup(movie_page.content, \"html.parser\")\n",
    "    \n",
    "    # Obtener director y estrellas\n",
    "    credit_summary_items = movie_soup.select('div.credit_summary_item')\n",
    "    director = credit_summary_items[0].a.text if credit_summary_items else 'N/A'\n",
    "    \n",
    "    stars = []\n",
    "    if len(credit_summary_items) > 1:\n",
    "        stars_links = credit_summary_items[1].select('a')\n",
    "        stars = [star.text for star in stars_links if 'name' in star['href']]\n",
    "    \n",
    "    titles.append(title)\n",
    "    release_years.append(release_year)\n",
    "    directors_and_stars.append((director, stars))\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Release Year': release_years,\n",
    "    'Director': [ds[0] for ds in directors_and_stars],\n",
    "    'Stars': [', '.join(ds[1]) for ds in directors_and_stars]\n",
    "})\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 12 - Nombre de la película, año y un breve resumen de las 10 películas aleatorias top (IMDB) como un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(soup.find_all(\"span\", {\"class\": \"secondaryInfo\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No hay suficientes películas para seleccionar 10 aleatorias.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# URL del Top 250 de IMDb\n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "\n",
    "# Realizar la solicitud HTTP a la página\n",
    "response = requests.get(url)\n",
    "\n",
    "# Analizar el contenido de la página con BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Encontrar todos los elementos con las películas del top 250\n",
    "movie_elements = soup.select('td.titleColumn')\n",
    "\n",
    "# Inicializar listas para almacenar los datos\n",
    "titles = []\n",
    "years = []\n",
    "summaries = []\n",
    "\n",
    "# Extraer la información de cada película\n",
    "for movie_element in movie_elements:\n",
    "    title = movie_element.a.text\n",
    "    year = movie_element.span.text.strip('()')\n",
    "    \n",
    "    # Navegar a la página de la película para obtener el resumen\n",
    "    movie_url = 'https://www.imdb.com' + movie_element.a['href']\n",
    "    movie_page = requests.get(movie_url)\n",
    "    movie_soup = BeautifulSoup(movie_page.content, \"html.parser\")\n",
    "    \n",
    "    # Obtener el resumen de la película\n",
    "    summary_element = movie_soup.select_one('div.summary_text')\n",
    "    summary = summary_element.text.strip() if summary_element else 'N/A'\n",
    "    \n",
    "    titles.append(title)\n",
    "    years.append(year)\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Verificar si tenemos al menos 10 películas\n",
    "if len(titles) < 10:\n",
    "    print(\"Error: No hay suficientes películas para seleccionar 10 aleatorias.\")\n",
    "else:\n",
    "    # Seleccionar 10 películas aleatorias\n",
    "    indices = random.sample(range(len(titles)), 10)\n",
    "\n",
    "    # Crear el DataFrame con las 10 películas seleccionadas\n",
    "    df_random_movies = pd.DataFrame({\n",
    "        'Title': [titles[i] for i in indices],\n",
    "        'Year': [years[i] for i in indices],\n",
    "        'Summary': [summaries[i] for i in indices]\n",
    "    })\n",
    "\n",
    "    print(df_random_movies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 13 - Encontrar el reporte meteorológico en vivo (temperatura, velocidad del viento, descripción y clima) de una ciudad dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al obtener los datos del clima.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def weather(city):\n",
    "    api_key = \"TU_CLAVE_API\"  # Reemplaza \"TU_CLAVE_API\" con tu clave de API de OpenWeatherMap\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "\n",
    "    # Parámetros de la solicitud\n",
    "    params = {\n",
    "        'q': city,\n",
    "        'appid': api_key,\n",
    "        'units': 'metric',  # Para obtener la temperatura en grados Celsius\n",
    "        'lang': 'es'  # Para obtener la descripción del clima en español\n",
    "    }\n",
    "\n",
    "    # Realizar la solicitud HTTP a la API de OpenWeatherMap\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    # Verificar si la solicitud fue exitosa\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extraer la información relevante\n",
    "        temperature = data['main']['temp']\n",
    "        wind_speed = data['wind']['speed']\n",
    "        weather_description = data['weather'][0]['description']\n",
    "        weather_main = data['weather'][0]['main']\n",
    "        \n",
    "        # Imprimir la información\n",
    "        print(f\"Ciudad: {city}\")\n",
    "        print(f\"Temperatura: {temperature}°C\")\n",
    "        print(f\"Velocidad del viento: {wind_speed} m/s\")\n",
    "        print(f\"Descripción: {weather_description}\")\n",
    "        print(f\"Clima: {weather_main}\")\n",
    "    else:\n",
    "        print(\"Error al obtener los datos del clima.\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "city = \"Madrid\"\n",
    "weather(city)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 14 - Nombre del libro, precio y disponibilidad de stock como un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "#url14 = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title   Price Availability\n",
      "0                                A Light in the Attic  £51.77     In stock\n",
      "1                                  Tipping the Velvet  £53.74     In stock\n",
      "2                                          Soumission  £50.10     In stock\n",
      "3                                       Sharp Objects  £47.82     In stock\n",
      "4               Sapiens: A Brief History of Humankind  £54.23     In stock\n",
      "5                                     The Requiem Red  £22.65     In stock\n",
      "6   The Dirty Little Secrets of Getting Your Dream...  £33.34     In stock\n",
      "7   The Coming Woman: A Novel Based on the Life of...  £17.93     In stock\n",
      "8   The Boys in the Boat: Nine Americans and Their...  £22.60     In stock\n",
      "9                                     The Black Maria  £52.15     In stock\n",
      "10     Starving Hearts (Triangular Trade Trilogy, #1)  £13.99     In stock\n",
      "11                              Shakespeare's Sonnets  £20.66     In stock\n",
      "12                                        Set Me Free  £17.46     In stock\n",
      "13  Scott Pilgrim's Precious Little Life (Scott Pi...  £52.29     In stock\n",
      "14                          Rip it Up and Start Again  £35.02     In stock\n",
      "15  Our Band Could Be Your Life: Scenes from the A...  £57.25     In stock\n",
      "16                                               Olio  £23.88     In stock\n",
      "17  Mesaerion: The Best Science Fiction Stories 18...  £37.59     In stock\n",
      "18                       Libertarianism for Beginners  £51.33     In stock\n",
      "19                            It's Only the Himalayas  £45.17     In stock\n"
     ]
    }
   ],
   "source": [
    "# Tu código aquí\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL de la página principal del sitio web de libros ficticio\n",
    "url14 = 'http://books.toscrape.com/'\n",
    "\n",
    "# Realizar la solicitud HTTP a la página\n",
    "response = requests.get(url14)\n",
    "\n",
    "# Analizar el contenido de la página con BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Inicializar listas para almacenar los datos\n",
    "book_titles = []\n",
    "book_prices = []\n",
    "book_availabilities = []\n",
    "\n",
    "# Encontrar todos los elementos con la información de los libros\n",
    "books = soup.select('article.product_pod')\n",
    "\n",
    "for book in books:\n",
    "    # Extraer el título del libro\n",
    "    title = book.h3.a['title']\n",
    "    \n",
    "    # Extraer el precio del libro\n",
    "    price = book.select_one('p.price_color').text\n",
    "    \n",
    "    # Extraer la disponibilidad del libro\n",
    "    availability = book.select_one('p.instock.availability').text.strip()\n",
    "    \n",
    "    # Añadir la información a las listas\n",
    "    book_titles.append(title)\n",
    "    book_prices.append(price)\n",
    "    book_availabilities.append(availability)\n",
    "\n",
    "# Crear el DataFrame con la información de los libros\n",
    "df_books = pd.DataFrame({\n",
    "    'Title': book_titles,\n",
    "    'Price': book_prices,\n",
    "    'Availability': book_availabilities\n",
    "})\n",
    "\n",
    "print(df_books)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitates tu output? Gracias! 🙂**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
